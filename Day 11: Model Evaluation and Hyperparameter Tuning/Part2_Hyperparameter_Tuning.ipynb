{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nepal-College-of-Information-Technology/AI-Data-Science-Workshop-2024/blob/main/Day%2011%3A%20Model%20Evaluation%20and%20Hyperparameter%20Tuning/Part2_Hyperparameter_Tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hyperparameter Tuning\n",
    "\n",
    "In this notebook, we will learn how to improve model performance by tuning the **hyperparameters**. Specifically, we will perform **grid search** to find the best hyperparameters for a **RandomForestClassifier**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Libraries and Load Dataset\n",
    "\n",
    "We will use the **breast cancer dataset** from `sklearn.datasets` as the dataset for this hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Define the Hyperparameter Grid\n",
    "\n",
    "In **grid search**, we define a set of hyperparameters that we want to tune. The grid search algorithm will then train the model on each combination of these hyperparameters and select the best one based on model performance.\n",
    "\n",
    "In this case, we will tune the following hyperparameters for the **RandomForestClassifier**:\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest.\n",
    "2. **max_depth**: The maximum depth of the tree.\n",
    "3. **min_samples_split**: The minimum number of samples required to split an internal node.\n",
    "4. **min_samples_leaf**: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Perform Grid Search for Hyperparameter Tuning\n",
    "\n",
    "We will use **GridSearchCV** to perform grid search with 5-fold cross-validation. This will help us find the best combination of hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Evaluate the Optimized Model\n",
    "\n",
    "Once we have the best hyperparameters from the grid search, we will evaluate the performance of the optimized model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Optimized Hyperparameters: 0.9649\n",
      "Classification Report for Optimized Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with Optimized Hyperparameters: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions and evaluate other metrics (precision, recall, f1-score)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_optimized = best_model.predict(X_test)\n",
    "print(\"Classification Report for Optimized Model:\")\n",
    "print(classification_report(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this notebook, we performed **hyperparameter tuning** using **GridSearchCV** to optimize a **RandomForestClassifier**. We found the best combination of hyperparameters that improved the model's performance on the test set. \n",
    "\n",
    "This approach ensures that we get the most out of the model without overfitting or underfitting by tuning the hyperparameters systematically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
